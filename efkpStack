


# Step 1: init cluster 
sudo kubeadm init --pod-network-cidr=192.168.0.0 /16 --apiserver-advertise-address=192.168.1.65

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# remove taints from control-plane
kubectl taint node wut node-role.kubernetes.io/control-plane:NoSchedule-

# set control-plane label
kubectl label nodes wut role=control-plane

kubectl cluster-info


# Step 2: cni plugin
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml
watch kubectl get pods -n calico-system


# nginx ingress
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.4/deploy/static/provider/cloud/deploy.yaml
kubectl apply  -f deploy.yaml

# create monitoring namespace
kubectl create namespace monitoring

# prometheus-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: prometheus.example.com  # hostname of service
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus
            port:
              number: 9090

kubectl apply -f prometheus-ingress.yml


# kibana-ingress.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: kibana.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana
            port:
              number: 5601

kubectl apply -f kibana-ingress.yaml

# verify the dns records for both these ingreses are up and pointing to the external ip of th eingress controller:
kubectl get svc -n ingress-nginx
kubectl get ds --all-namespaces | grep nginx


# Step 4: deploy logging, monitoring, and metrics 

# #########################
# depoy kubernetes metrics server 
# #########################

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
# --kubelet-insecure-tls

# persistentvolume.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kibana-pv
spec:
    capacity:
        storage: 5Gi
    accessModes:
        - ReadWriteOnce
    hostPath:
        path: /data/kibana

---

# persistentvolumeclaim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: kibana-pvc
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 5Gi
    selector:
        matchLabels:
            app: kibana

# Apply the PV and PVC
kubectl apply -f persistentvolume.yaml
kubectl apply -f persistentvolumeclaim.yaml



# #########################
# deploy kibana and apply the Kibana and service
# #########################

# kibana-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: monitoring
spec:
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
    name: http

kubectl apply -f kibana-service.yaml

# kibana.yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana
spec:
  version: 8.12.1
  count: 1
  elasticsearchRef:
    name: kibana


kubectl apply -f https://raw.githubusercontent.com/elastic/cloud-on-k8s/master/config/samples/kibana/kibana.yaml --namespace=monitoring


# #########################
# Deplpoy fluentd and apply the Fluentd Elasticsearch plugin configuration. You might need to create a configuration file that specifies the Elasticsearch host, among other setting
# #########################

# fluentd-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-system   # <--- Fluentd typically needs cluster-wide access to collect logs

# fluentd-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
  - apiGroups: [""]
    resources:
      - pods
      - namespaces
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io


# fluentd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: kube-system
data:
  fluent.conf: |
    <match **>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host elasticsearch.monitoring.svc.cluster.local
      port 9200
      logstash_format true
    </match>


# allow-fluentd-to-elasticsearch.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-fluentd-to-elasticsearch
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app: elasticsearch
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: fluentd
  policyTypes:
  - Ingress

# deploy the Fluentd DaemonSet with the Elasticsearch plugin configuration
kubectl apply -f allow-fluentd-to-elasticsearch.yaml
kubectl apply -f fluentd-serviceaccount.yaml
kubectl apply -f fluentd-rbac.yaml
kubectl apply -f fluentd-configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml

# #########################
# deploy prometheus and apply the prometheus service
# #########################

# prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30090
  selector:
    app: prometheus

# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https


kubectl apply -f prometheus-service.yaml
kubectl apply -f prometheus-configmap.yaml

# deploy node-explorer and service

# node-explorer.yaml
node-explorer.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: node-exporter
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/name: node-exporter
  template:
    metadata:
      labels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: node-exporter
    spec:
      containers:
      - args:
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --no-collector.wifi
        - --no-collector.hwmon
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)
        - --collector.netclass.ignored-devices=^(veth.*)$
        name: node-exporter
        image: prom/node-exporter
        ports:
          - containerPort: 9100
            protocol: TCP
        resources:
          limits:
            cpu: 250m
            memory: 180Mi
          requests:
            cpu: 102m
            memory: 180Mi
        volumeMounts:
        - mountPath: /host/sys
          mountPropagation: HostToContainer
          name: sys
          readOnly: true
        - mountPath: /host/root
          mountPropagation: HostToContainer
          name: root
          readOnly: true
      volumes:
      - hostPath:
          path: /sys
        name: sys
      - hostPath:
          path: /
        name: root

# node-explorer-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: node-exporter
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9100'
spec:
  type: NodePort
  selector:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/name: node-exporter
  ports:
  - name: node-exporter
    protocol: TCP
    port: 9100
    # targetPort: 9100

kubectl apply -f node-explorer.yaml
kubectl apply -f node-explorer-service.yaml

# verify services, roles, pods, etc
kubectl get service --namespace=monitoring
kubectl get roles,rolebindings --namespace=monitoring        
kubectl get pods --namespace=monitoring 

# Step 5: deploy monitoring network policis

# monitoring-network-policies.yaml
# allow-fluentd-to-elasticsearch
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-fluentd-to-elasticsearch
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app: elasticsearch
  ingress:
  - from:
    - podSelector:
        matchLabels:
          k8s-app: fluentd

# allow-prometheus-scrape
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus-scrape
  namespace: monitoring
spec:
  podSelector: {}
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring

# allow-internal-access-to-kibana
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-internal-access-to-kibana
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app: kibana
  ingress:
  - from: []

#  allow-external-access-to-kibana
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-access-to-kibana
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app: kibana
  ingress:
    - from: []
  policyTypes:
    - Ingress

kubectl apply -f monitoring-network-policies.yaml
